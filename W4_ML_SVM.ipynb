{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Perform exploratory  Data Analysis and determine Training Labels\n\n# *   create a column for the class\n# *   Standardize the data\n# *   Split into training data and test data\n\n# \\-Find best Hyperparameter for SVM, Classification Trees and Logistic Regression\n\n# *   Find the method performs best using test data\n\nimport piplite\nawait piplite.install(['numpy'])\nawait piplite.install(['pandas'])\nawait piplite.install(['seaborn'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\nimport pandas as pd\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\nimport numpy as np\n# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\nimport matplotlib.pyplot as plt\n#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\nimport seaborn as sns\n# Preprocessing allows us to standarsize our data\nfrom sklearn import preprocessing\n# Allows us to split our data into training and testing data\nfrom sklearn.model_selection import train_test_split\n# Allows us to test parameters of classification algorithms and find the best one\nfrom sklearn.model_selection import GridSearchCV\n# Logistic Regression classification algorithm\nfrom sklearn.linear_model import LogisticRegression\n# Support Vector Machine classification algorithm\nfrom sklearn.svm import SVC\n# Decision Tree classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n# K Nearest Neighbors classification algorithm\nfrom sklearn.neighbors import KNeighborsClassifier",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "def plot_confusion_matrix(y,y_predict):\n    \"this function plots the confusion matrix\"\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(y, y_predict)\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed']) \n    plt.show() ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "#Load the dataframe\nfrom js import fetch\nimport io\n\nURL1 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\"\nresp1 = await fetch(URL1)\ntext1 = io.BytesIO((await resp1.arrayBuffer()).to_py())\ndata = pd.read_csv(text1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "data.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "URL2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv'\nresp2 = await fetch(URL2)\ntext2 = io.BytesIO((await resp2.arrayBuffer()).to_py())\nX = pd.read_csv(text2)\nX.head(100)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 1\n# Create a NumPy array from the column Class in data, by applying the method to_numpy() then assign it to the variable Y,make sure the output is a Pandas series (only one bracket df['name of column']).\nimport numpy as np\n\n# Assuming 'data' is your DataFrame and 'Class' is the column name\nY = data['Class'].to_numpy()\n\n# Verify the type of Y\nprint(type(Y))  # This should output <class 'numpy.ndarray'>\n\n# TASK 2\n# Standardize the data in X then reassign it to the variable X using the transform provided below.\n\n# # students get this \ntransform = preprocessing.StandardScaler()\n# We split the data into training and testing data using the function train_test_split. The training data is divided into validation data, a second set used for training data; then the models are trained and hyperparameters are selected using the function GridSearchCV.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming X is your DataFrame and y is your target variable\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Standardize the training data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\n\n# Standardize the testing data using the same scaler\nX_test = scaler.transform(X_test)\n\n# TASK 3\n# Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2. The training data and test data should be assigned to the following labels.\n\n#from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\nprint(\"Number of training samples:\", len(X_train))\nprint(\"Number of test samples:\", len(X_test))\nY_test.shape\n\n# TASK 4\n# Create a logistic regression object then create a GridSearchCV object logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a logistic regression object\nlogreg = LogisticRegression()\n\n# Define the parameters for grid search\nparameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Create a GridSearchCV object\nlogreg_cv = GridSearchCV(logreg, parameters, cv=10)\n\n# Fit the GridSearchCV object to the data\nlogreg_cv.fit(X_train, Y_train)\n\n# Output the best parameters and accuracy\nprint(\"Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\nprint(\"Accuracy: \", logreg_cv.best_score_)\n\n#TASK 5\n#Calculate the accuracy on the test data using the method score:\n#Lets look at the confusion matrix:\n\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Calculate the accuracy on the test data\naccuracy = logreg_cv.score(X_test, Y_test)\nprint(\"Accuracy on test data:\", accuracy)\n\n# Generate predictions on the test data\nyhat = logreg_cv.predict(X_test)\n\n# Plot the confusion matrix\nplot_confusion_matrix(logreg_cv, X_test, Y_test)\nplot_confusion_matrix(Y_test,yhat)\nplt.title('Confusion Matrix')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 6\n# Create a support vector machine object then create a GridSearchCV object svm_cv with cv - 10. Fit the object to find the best parameters from the dictionary parameters.\n# parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n#               'C': np.logspace(-3, 3, 5),\n#               'gamma':np.logspace(-3, 3, 5)}\n# svm = SVC()\n# print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n# print(\"accuracy :\",svm_cv.best_score_)\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\n\n# Create an SVM object\nsvm = SVC()\n\n# Define the parameters for grid search\nparameters = {\n    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n    'C': np.logspace(-3, 3, 5),\n    'gamma': np.logspace(-3, 3, 5)\n}\n\n# Create a GridSearchCV object\nsvm_cv = GridSearchCV(svm, parameters, cv=10)\n\n# Fit the GridSearchCV object to the data\nsvm_cv.fit(X_train, Y_train)\n\n# Output the best parameters and accuracy\nprint(\"Tuned hyperparameters (best parameters): \", svm_cv.best_params_)\nprint(\"Accuracy: \", svm_cv.best_score_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 7\n# Calculate the accuracy on the test data using the method score:\n# Calculate the accuracy on the test data\naccuracy = svm_cv.score(X_test, Y_test)\nprint(\"Accuracy on test data:\", accuracy)\n#We can plot the confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Plot the confusion matrix\nplot_confusion_matrix(svm_cv, X_test, Y_test)\nplt.title('Confusion Matrix')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 8\n# Create a decision tree classifier object then create a GridSearchCV object tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a Decision Tree Classifier object\ntree_clf = DecisionTreeClassifier()\n\n# Define the parameters for grid search\nparameters = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 10, 20, 30, 40, 50],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Create a GridSearchCV object\ntree_cv = GridSearchCV(tree_clf, parameters, cv=10)\n\n# Fit the GridSearchCV object to the data\ntree_cv.fit(X_train, Y_train)\n\n# Output the best parameters and accuracy\nprint(\"Tuned hyperparameters (best parameters): \", tree_cv.best_params_)\nprint(\"Accuracy: \", tree_cv.best_score_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 9\n# Calculate the accuracy of tree_cv on the test data using the method score:\n# We can plot the confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Calculate the accuracy on the test data\naccuracy = tree_cv.score(X_test, Y_test)\nprint(\"Accuracy on test data:\", accuracy)\n\n# Plot the confusion matrix\nplot_confusion_matrix(tree_cv, X_test, Y_test)\nplt.title('Confusion Matrix')\nplt.show()\n\n# yhat = tree_cv.predict(X_test)\n# plot_confusion_matrix(Y_test,yhat)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 10\n# Create a k nearest neighbors object then create a GridSearchCV object knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a KNN object\nknn = KNeighborsClassifier()\n\n# Define the parameters for grid search\nparameters = {\n    'n_neighbors': [3, 5, 7, 9],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\n# Create a GridSearchCV object\nknn_cv = GridSearchCV(knn, parameters, cv=10)\n\n# Fit the GridSearchCV object to the data\nknn_cv.fit(X_train, Y_train)\n\n# Output the best parameters and accuracy\nprint(\"Tuned hyperparameters (best parameters): \", knn_cv.best_params_)\nprint(\"Accuracy: \", knn_cv.best_score_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 11\n# Calculate the accuracy of knn_cv on the test data using the method score:\n# Calculate the accuracy on the test data\naccuracy = knn_cv.score(X_test, Y_test)\nprint(\"Accuracy on test data:\", accuracy)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# TASK 12\n# Find the method performs best:\n# Calculate the accuracy of each model on the test data\naccuracy_lr = logreg_cv.score(X_test, Y_test)\naccuracy_svm = svm_cv.score(X_test, Y_test)\naccuracy_tree = tree_cv.score(X_test, Y_test)\naccuracy_knn = knn_cv.score(X_test, Y_test)\n\n# Print the accuracy scores\nprint(\"Logistic Regression Accuracy:\", accuracy_lr)\nprint(\"Support Vector Machine Accuracy:\", accuracy_svm)\nprint(\"Decision Tree Accuracy:\", accuracy_tree)\nprint(\"K-Nearest Neighbors Accuracy:\", accuracy_knn)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}